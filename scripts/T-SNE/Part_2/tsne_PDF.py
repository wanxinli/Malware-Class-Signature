#
#  tsne.py
#
# Implementation of t-SNE in Python. The implementation was tested on Python 2.7.10, and it requires a working
# installation of NumPy. The implementation comes with an example on the MNIST dataset. In order to plot the
# results of this example, a working installation of matplotlib is required.
#
# The example can be run by executing: `ipython tsne.py`
#
#
#  Created by Laurens van der Maaten on 20-12-08.
#  Copyright (c) 2008 Tilburg University. All rights reserved.

import sys
import numpy
import sklearn.preprocessing
import sklearn.utils.extmath
import subprocess

import matplotlib.pyplot as pyplot
import matplotlib.gridspec as gridspec
from matplotlib import pyplot as plot
from matplotlib.backends.backend_pdf import PdfPages
from collections import Counter


def Hbeta(D, beta = 1.0):
	"""Compute the perplexity and the P-row for a specific value of the precision of a Gaussian distribution."""

	# Compute P-row and corresponding perplexity
	lse = sklearn.utils.extmath.logsumexp(-D * beta)
	explse = numpy.exp(lse, dtype=numpy.float128)
	P = numpy.exp(-D * beta)
	H = lse + beta * numpy.exp( numpy.log(numpy.sum(D * P)) - lse )
	P = numpy.exp( numpy.log(P)-lse )
	return H, P


def x2p(X, tol = 1e-5, perplexity = 30.0):
	"""Performs a binary search to get P-values in such a way that each conditional Gaussian has the same perplexity."""

	# Initialize some variables
	print "Computing pairwise distances..."
	(n, d) = X.shape
	sum_X = numpy.sum(X**2, axis=1)
	D = numpy.add(numpy.add(-2 * numpy.dot(X, X.T), sum_X).T, sum_X)
	P = numpy.zeros((n, n))
	beta = numpy.ones((n, 1))
	logU = numpy.log(perplexity)

	# Loop over all datapoints
	for i in range(n):

		# Print progress
		if i % 500 == 0:
			print "Computing P-values for point ", i, " of ", n, "..."

		# Compute the Gaussian kernel and entropy for the current precision
		betamin = -numpy.inf
		betamax =  numpy.inf
		Di = D[i, numpy.concatenate((numpy.r_[0:i], numpy.r_[i+1:n]))]
		(H, thisP) = Hbeta(Di, beta[i])

		# Evaluate whether the perplexity is within tolerance
		Hdiff = H - logU
		tries = 0
		while numpy.abs(Hdiff) > tol and tries < 50:

			# If not, increase or decrease precision
			if Hdiff > 0:
				betamin = beta[i].copy()
				if betamax == numpy.inf or betamax == -numpy.inf:
					beta[i] = beta[i] * 2
				else:
					beta[i] = (beta[i] + betamax) / 2
			else:
				betamax = beta[i].copy()
				if betamin == numpy.inf or betamin == -numpy.inf:
					beta[i] = beta[i] / 2
				else:
					beta[i] = (beta[i] + betamin) / 2

			# Recompute the values
			(H, thisP) = Hbeta(Di, beta[i])
			Hdiff = H - logU
			tries = tries + 1

		# Set the final row of P
		P[i, numpy.concatenate((numpy.r_[0:i], numpy.r_[i+1:n]))] = thisP

	# Return final P-matrix
	print "Mean value of sigma: ", numpy.mean(numpy.sqrt(1 / beta))
	return P


def pca(X, no_dims = 50):
	"""Runs PCA on the NxD array X in order to reduce its dimensionality to no_dims dimensions."""

	print "Preprocessing the data using PCA..."
	(n, d) = X.shape
	X = X - numpy.tile(numpy.mean(X, 0), (n, 1))
	(l, M) = numpy.linalg.eig(numpy.dot(X.T, X))
	Y = numpy.dot(X, M[:,0:no_dims])
	return Y.real


def tsne(X, no_dims=2, initial_dims=50, perplexity=30.0, max_iter=1000):
	"""Runs t-SNE on the dataset in the NxD array X to reduce its dimensionality to no_dims dimensions.
	The syntaxis of the function is Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array."""

	# Check inputs
	if isinstance(no_dims, float):
		print "Error: array X should have type float."
		return -1
	if round(no_dims) != no_dims:
		print "Error: number of dimensions should be an integer."
		return -1

	# Initialize variables
	X = pca(X, initial_dims)
	(n, d) = X.shape
	initial_momentum = 0.5
	final_momentum = 0.8
	eta = 500
	min_gain = 0.01
	Y = numpy.random.randn(n, no_dims)
	dY = numpy.zeros((n, no_dims))
	iY = numpy.zeros((n, no_dims))
	gains = numpy.ones((n, no_dims))

	# Compute P-values
	P = x2p(X, 1e-5, perplexity)
	P = P + numpy.transpose(P)
	P = P / numpy.sum(P)
	P = P * 4 # early exaggeration
	P = numpy.maximum(P, 1e-12)

	# Run iterations
	for iter in range(max_iter):

		# Compute pairwise affinities
		sum_Y = numpy.sum(numpy.square(Y), 1)
		num = 1 / (1 + numpy.add(numpy.add(-2 * numpy.dot(Y, Y.T), sum_Y).T, sum_Y))
		num[range(n), range(n)] = 0
		Q = num / numpy.sum(num)
		Q = numpy.maximum(Q, 1e-12)

		# Compute gradient
		PQ = P - Q
		for i in range(n):
			dY[i,:] = numpy.sum(numpy.tile(PQ[:,i] * num[:,i], (no_dims, 1)).T * (Y[i,:] - Y), 0)

		# Perform the update
		if iter < 20:
			momentum = initial_momentum
		else:
			momentum = final_momentum
		gains = (gains + 0.2) * ((dY > 0) != (iY > 0)) + (gains * 0.8) * ((dY > 0) == (iY > 0))
		gains[gains < min_gain] = min_gain
		iY = momentum * iY - eta * (gains * dY)
		Y = Y + iY
		Y = Y - numpy.tile(numpy.mean(Y, 0), (n, 1))

		# Compute current value of cost function
		if (iter + 1) % 10 == 0:
			C = numpy.sum(P * numpy.log(P / Q))
			print "Iteration ", (iter + 1), ": error is ", C
 		# Stop lying about P-values
		if iter == 100:
			P = P / 4

	# Return solution
	return Y


if __name__ == "__main__":
	numpy.seterr(divide='ignore', invalid='ignore')

	# Get features for each item
	with open(sys.argv[1],'r') as F:
		data = F.readlines()
		header = data[0][1:] # Features names
		data = data[1:]
		data = map(lambda l: l.split('\n')[0].split(','), data)
		data = { r[0] : map(float,r[1:]) for r in data }

	# Get labels for each item
	with open(sys.argv[2],'r') as F:
		labels = F.readlines()
		labels = map(lambda l: l.split('\n')[0].split(','), labels)
		labels = { r[0] : r[1] for r in labels }
		idx = { l : i for (i,l) in enumerate(list(set(labels.values()))) }

	assert len(data) == len(labels)

	# Make sure features and labels are in the same order (and matching)
	X = list()
	L = list()
	for (h,x) in data.iteritems():
		assert h in labels
		X.append(x)
		L.append(idx[labels[h]])

	X = numpy.array(X)
        pdf_pages = PdfPages('t-sne.pdf')
	# Preprocessing features
#	d = X.max(axis=0) - X.min(axis=0)
#	X = X[:,d > 0]
#	X = numpy.log(X+1.)
#	X = sklearn.preprocessing.scale(X)

#	Y = tsne(X, no_dims=2, initial_dims=50, perplexity=20.0, max_iter=1000)
        print len(set(L))

        varIter = input('\n  Please enter the maximum number of iteration \n  for the optimization. Should be at least (200)\n  recommended ( 1000 )  \n  >> ')
        varPlex = input('\n  Please enter the number of the perplexity  which is related \n  to the number of nearest neighbors. \n  Consider selecting a value between (5) and (50)\n  recommended ( 20 )  \n  >> ') 
        visibleColor = raw_input('\n  Please Enter a color for the selected cluster points.\n  Use any color defined in Matplotlib\n  recommended ( red or blue )   \n  >> ')
        nonvisibleColor = raw_input('\n  For other clusters points, Please Enter DIFFERENT color\n  recommended ( lightgray or linen )  \n  >> ')


	Y = tsne(X, no_dims=2, initial_dims=50, perplexity=varPlex, max_iter=varIter)
        valueOfDict = []
        valueOfDict = labels.values()
        valueOfDict = map(int, valueOfDict)

 
        for j in range (1,len(set(L))+1):
                for i in range (0,len(valueOfDict)):
                        if valueOfDict[i] == j:
                            pyplot.scatter(Y[i,0], Y[i,1], 20, visibleColor ,edgecolor=None);
                        else:
                            pyplot.scatter(Y[i,0], Y[i,1], 20, nonvisibleColor ,edgecolor=None);
                pyplot.title ('TSNE result of CLUSTER #%d' % j )
                fig = pyplot.savefig('images/tsne_%d.png' % j)
                pdf_pages.savefig(fig)
        pdf_pages.close()
        print 'Images are successfully generated ... '
        print 'PDF file is also successfully generated ... '
        print 'Done'

